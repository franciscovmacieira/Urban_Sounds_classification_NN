{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary \n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       # 4 convolutional blocks / flatten the results / linear layer / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, \n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1, \n",
    "                padding=2\n",
    "            ), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1, \n",
    "                padding=2\n",
    "            ), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, \n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1, \n",
    "                padding=2\n",
    "            ), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, \n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1, \n",
    "                padding=2\n",
    "            ), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 5 * 4, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        prediction = self.softmax(logits)\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 66, 46]             160\n",
      "              ReLU-2           [-1, 16, 66, 46]               0\n",
      "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
      "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
      "              ReLU-5           [-1, 32, 35, 25]               0\n",
      "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
      "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
      "              ReLU-8           [-1, 64, 19, 14]               0\n",
      "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
      "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
      "             ReLU-11           [-1, 128, 11, 9]               0\n",
      "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
      "          Flatten-13                 [-1, 2560]               0\n",
      "           Linear-14                   [-1, 10]          25,610\n",
      "          Softmax-15                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 122,762\n",
      "Trainable params: 122,762\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.83\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 2.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cnn = CNNNetwork()\n",
    "    cnn = cnn.to('cpu')  \n",
    "    summary(cnn, (1, 64, 44))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    mfcc = librosa.feature.vfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "\n",
    "    mfcc = np.mean(mfcc, axis=1)\n",
    "\n",
    "    return mfcc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = np.expand_dims(mfcc, axis=0)\n",
    "mfcc = np.expand_dims(mfcc, axis=0)\n",
    "\n",
    "mfcc_tensor = torch.tensor(mfcc, dtype=torch.float32)\n",
    "\n",
    "print(mfcc_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        features = extract_features(file_path)\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float23)\n",
    "\n",
    "        features = features.unsqueeze(0)\n",
    "\n",
    "        features = features.unsqueeze(0)\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "file_paths = glob('/Users/gabrielasimon/Desktop/ac2/UrbanSound8K/audio/*/*.wav')\n",
    "labels = [0, 1, 0]\n",
    "\n",
    "dataset = AudioDataset(file_paths=file_paths, labels=labels)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for input, targets, in data_loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
