{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca523eb",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df4cc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df468b9",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d2fcf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('iris.csv')\n",
    "\n",
    "features = data.drop(columns=['class'])\n",
    "labels = data['class']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.30, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b212d6a7",
   "metadata": {},
   "source": [
    "Codificação do y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bb83c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_val_encoded = encoder.transform(y_val.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be45a16",
   "metadata": {},
   "source": [
    "\"Currently, we do not usually use the sigmoid function for the hidden layers in MLPs and CNNs. Instead, we use ReLU or Leaky ReLU there.\"  \n",
    "\"We do not usually use the sigmoid function in the hidden layers because of the following drawbacks.\n",
    "The sigmoid function has the vanishing gradient problem. This is also known as saturation of the gradients.\n",
    "The sigmoid function has slow convergence.\n",
    "Its outputs are not zero-centered. Therefore, it makes the optimization process harder.\n",
    "This function is computationally expensive as an e^z term is included.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7343a",
   "metadata": {},
   "source": [
    "ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73b7ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_act(x, der=False):\n",
    "    if der:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    else:\n",
    "        return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf4e15",
   "metadata": {},
   "source": [
    "\"Inventing ReLU is one of the most important breakthroughs made in deep learning.\n",
    "This function does not have the vanishing gradient problem.  \n",
    "This function is computationally inexpensive. It is considered that the convergence of ReLU is 6 times faster than sigmoid and tanh functions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422c425",
   "metadata": {},
   "source": [
    "Derivada da função ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93b65b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9f47c",
   "metadata": {},
   "source": [
    "SoftMax activtion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e1b610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_act(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb175a13",
   "metadata": {},
   "source": [
    "Duas camadas escondidas já são capazes de representar qualquer relação entre os dados, mesmo aquelas que não podem ser representadas por equações. Mais do que duas camadas escondidas só são necessárias em problemas ainda mais complexos como séries temporais e visão computacional, onde há uma certa inter-relação entre as dimensões que os dados contêm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95668562",
   "metadata": {},
   "source": [
    "Leaky ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26073467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_ReLU_act(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5077938",
   "metadata": {},
   "source": [
    "\"If the input value is 0 greater than 0, the leaky ReLU function outputs the input as it is like the default ReLU function does. However, if the input is less than 0, the leaky ReLU function outputs a small negative value defined by αz (where α is a small constant value, usually 0.01 and z is the input value).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff978b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range do nº de camadas:  [2, 3]\n"
     ]
    }
   ],
   "source": [
    "n_camadas = [2, 3]\n",
    "print (\"Range do nº de camadas: \", n_camadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bef14c",
   "metadata": {},
   "source": [
    "Abordagens:  \n",
    "O número de neurônios escondidos deve estar entre o tamanho da camada de entrada e o da camada de saída.  \n",
    "O número de neurônios escondidos deve ser 2/3 do tamanho da camada de entrada, mais o tamanho da camada de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d15eb126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de neurónios escondidos:  [4, 6]\n"
     ]
    }
   ],
   "source": [
    "n_neur = [0] * 2\n",
    "n_neur [0] = int ((X_train.shape[1] + y_train_encoded.shape[1])/2)\n",
    "n_neur [1] = int(((X_train.shape[1]*2)/3) + y_train_encoded.shape[1])\n",
    "print (\"Número de neurónios escondidos: \", n_neur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea0cce",
   "metadata": {},
   "source": [
    "\"We should use a non-linear activation function in hidden layers. The choice is made by considering the performance of the model or convergence of the loss function. Start with the ReLU activation function and if you have a dying ReLU problem, try leaky ReLU.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4101b",
   "metadata": {},
   "source": [
    "ADAM Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5a67463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptim():\n",
    "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "\n",
    "    def update(self, t, w, dw):\n",
    "        m_dw = np.zeros_like(w)  # Inicializa com zeros com a mesma forma de w\n",
    "        v_dw = np.zeros_like(w)  # Inicializa com zeros com a mesma forma de w\n",
    "        \n",
    "        if dw.shape != w.shape:\n",
    "            raise ValueError(f\"Forma de gradiente {dw.shape} não corresponde à forma de pesos {w.shape}\")\n",
    "\n",
    "        # Cópia dos pesos originais antes da atualização\n",
    "        updated_w = np.copy(w)\n",
    "\n",
    "        # Momentum beta 1\n",
    "        m_dw = self.beta1 * m_dw + (1 - self.beta1) * dw\n",
    "\n",
    "        # RMS beta 2\n",
    "        v_dw = self.beta2 * v_dw + (1 - self.beta2) * (dw ** 2)\n",
    "\n",
    "        # Bias correction\n",
    "        m_dw_corr = m_dw / (1 - self.beta1 ** t)\n",
    "        v_dw_corr = v_dw / (1 - self.beta2 ** t)\n",
    "\n",
    "        # Atualizar pesos\n",
    "        updated_w -= self.eta * (m_dw_corr / (np.sqrt(v_dw_corr) + self.epsilon))\n",
    "        return updated_w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16d95e",
   "metadata": {},
   "source": [
    "Cálculo dos gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "4e158f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "5c758f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, theta):\n",
    "    h = hypothesis(X, theta)\n",
    "    grad = np.dot(X.T, (h - y))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d9c8bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, theta):\n",
    "    h = hypothesis(X, theta)\n",
    "    J = np.dot((h - y).T, (h - y)) / 2\n",
    "    return J[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e41cb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X, y, batch_size):\n",
    "    mini_batches = []\n",
    "    data = np.hstack((X, y))\n",
    "    np.random.shuffle(data)\n",
    "    n_minibatches = data.shape[0] // batch_size\n",
    "    i = 0\n",
    "    for i in range(n_minibatches + 1):\n",
    "        mini_batch = data[i * batch_size:(i + 1) * batch_size, :]\n",
    "        X_mini = mini_batch[:, :-1]\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        mini_batch = data[i * batch_size:data.shape[0]]\n",
    "        X_mini = mini_batch[:, :-1]\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "d912fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, learning_rate=0.001, batch_size=32):\n",
    "    theta = np.zeros((X.shape[1], 1))\n",
    "    error_list = []\n",
    "    max_iters = 3\n",
    "    adam = AdamOptim(learning_rate)\n",
    "    t = 0\n",
    "    for itr in range(max_iters):\n",
    "        mini_batches = create_mini_batches(X, y, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, y_mini = mini_batch\n",
    "            grad = gradient(X_mini, y_mini, theta)\n",
    "            t += 1\n",
    "            theta = adam.update(t, theta, grad)\n",
    "            error_list.append(cost(X_mini, y_mini, theta))\n",
    "    return theta, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f487a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(x, y_true, parameters):\n",
    "    # Obter pesos e biases\n",
    "    W1, b1, W2, b2, W3, b3 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2'], parameters['W3'], parameters['b3']\n",
    "    \n",
    "    # Forward pass\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = ReLU_act(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = ReLU_act(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    y_pred = softmax_act(z3)\n",
    "    \n",
    "    # Backward pass\n",
    "    dz3 = y_pred - y_true\n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    db3 = np.sum(dz3, axis=1, keepdims=True)\n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    dz2 = da2 * ReLU_act(z2, der=True)\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True)\n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    dz1 = da1 * ReLU_act(z1, der=True)\n",
    "    dW1 = np.dot(dz1, x.T)\n",
    "    db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\n",
    "        'dW1': dW1,\n",
    "        'db1': db1,\n",
    "        'dW2': dW2,\n",
    "        'db2': db2,\n",
    "        'dW3': dW3,\n",
    "        'db3': db3\n",
    "    }\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "20c6ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(w0, w1, threshold=1e-4):\n",
    "    print(np.linalg.norm(w0 - w1))\n",
    "    return np.linalg.norm(w0 - w1) < threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b29684",
   "metadata": {},
   "source": [
    "Inicialização dos pesos usando uma distribuição gaussiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "f486df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, mean=0.0, stddev=0.01):\n",
    "    return np.random.normal(loc=mean, scale=stddev, size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10423256",
   "metadata": {},
   "source": [
    "MLP com 2 camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "77968b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022360166577705315\n",
      "0.016639329408191388\n",
      "0.014282870422924728\n",
      "0.0\n",
      "Balanced Accuracy: 0.33\n",
      "Matriz de Confusão:\n",
      "[[ 0  6  0]\n",
      " [ 0 10  0]\n",
      " [ 0  7  0]]\n"
     ]
    }
   ],
   "source": [
    "p = n_neur[0]\n",
    "q = n_neur[1]\n",
    "num_classes = 3\n",
    "\n",
    "w1 = initialize_weights((p, X_train.shape[1]))\n",
    "b1 = initialize_weights((p, 1)) \n",
    "w2 = initialize_weights((q, p))\n",
    "b2 = initialize_weights((q, 1))  \n",
    "wOut = initialize_weights((num_classes, q))\n",
    "bOut = initialize_weights((num_classes, 1))\n",
    "\n",
    "# Inicializar parâmetros do ADAM\n",
    "adam1 = AdamOptim()\n",
    "adam2 = AdamOptim()\n",
    "adamOut = AdamOptim()\n",
    "t = 1\n",
    "\n",
    "# Inicializar parâmetros\n",
    "parameters = {\n",
    "    'W1': w1,\n",
    "    'b1': b1,\n",
    "    'W2': w2,\n",
    "    'b2': b2,\n",
    "    'W3': wOut,\n",
    "    'b3': bOut\n",
    "}\n",
    "\n",
    "# Treinar o modelo\n",
    "converged = False\n",
    "while not converged:\n",
    "    for i in range(0, X_train.shape[0]):\n",
    "        x = X_train[i].reshape(-1, 1)\n",
    "        y_true = y_train_encoded[i].reshape(-1, 1)\n",
    "\n",
    "        grads = compute_gradients(x, y_true, parameters)\n",
    "\n",
    "        w1_old = parameters['W1'].copy()\n",
    "        parameters['W1'] = adam1.update(t, parameters['W1'], grads['dW1'])\n",
    "        parameters['b1'] = adam1.update(t, parameters['b1'], grads['db1'])\n",
    "        parameters['W2'] = adam2.update(t, parameters['W2'], grads['dW2'])\n",
    "        parameters['b2'] = adam2.update(t, parameters['b2'], grads['db2'])\n",
    "        parameters['W3'] = adamOut.update(t, parameters['W3'], grads['dW3'])\n",
    "        parameters['b3'] = adamOut.update(t, parameters['b3'], grads['db3'])\n",
    "\n",
    "        if check_convergence(parameters['W1'], w1_old):\n",
    "            converged = True\n",
    "            break\n",
    "        else:\n",
    "            t += 1\n",
    "\n",
    "# Testar o modelo em X_test\n",
    "class_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "y_pred = np.array([np.argmax(softmax_act(np.dot(parameters['W3'], ReLU_act(np.dot(parameters['W2'], ReLU_act(np.dot(parameters['W1'], x.reshape(-1, 1)) + parameters['b1'])) + parameters['b2'])) + parameters['b3'])) for x in X_test])\n",
    "\n",
    "y_pred_labels = [class_names[pred] for pred in y_pred]\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred_labels)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.2f}\")\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7414b6",
   "metadata": {},
   "source": [
    "MLP com 3 camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a7df994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.33\n",
      "Matriz de Confusão:\n",
      "[[ 6  0  0]\n",
      " [10  0  0]\n",
      " [ 7  0  0]]\n",
      "Balanced Accuracy: 0.33\n",
      "Matriz de Confusão:\n",
      "[[ 0  0  6]\n",
      " [ 0  0 10]\n",
      " [ 0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "for j in range (2):\n",
    "    # Número de perceptrons por camada\n",
    "    p = n_neur[j]  # Layer 1\n",
    "    q = n_neur[j]\n",
    "    r = n_neur[j] # Layer 2\n",
    "    num_classes = 3  # Número de classes na previsão multiclasse\n",
    "\n",
    "    # Taxa de aprendizagem\n",
    "    eta = 1/623\n",
    "\n",
    "    # Inicializar pesos e biases\n",
    "    w1 = 2 * np.random.rand(p, X_train.shape[1]) - 0.5  # Layer 1\n",
    "    b1 = np.random.rand(p)\n",
    "    w2 = 2 * np.random.rand(q, p) - 0.5  # Layer 2\n",
    "    b2 = np.random.rand(q)\n",
    "    w3 = 2 * np.random.rand(q, p) - 0.5  # Layer 2\n",
    "    b3 = np.random.rand(q)\n",
    "    wOut = 3 * np.random.rand(num_classes, r) - 0.5  # Output Layer\n",
    "    bOut = np.random.rand(num_classes)\n",
    "\n",
    "    mu = []\n",
    "    vec_y = []\n",
    "    class_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "    # Loop sobre os dados de treino\n",
    "    for i in range(0, X_train.shape[0]):\n",
    "        # Input data\n",
    "        x = X_train[i]\n",
    "        y_true = y_train_encoded[i]\n",
    "\n",
    "        # Feedforward\n",
    "        z1 = ReLU_act(np.dot(w1, x) + b1)  # output layer 1\n",
    "        z2 = ReLU_act(np.dot(w2, z1) + b2)  # output layer 2\n",
    "        z3 = ReLU_act(np.dot(w3, z2) + b3)\n",
    "        y = softmax_act(np.dot(wOut, z3) + bOut)  # Output of the Output layer\n",
    "\n",
    "        # Compute the output layer's error (cross-entropy)\n",
    "        delta_Out = y - y_true\n",
    "\n",
    "        # Backpropagate\n",
    "        delta_3 = np.dot(delta_Out, wOut) * ReLU_act(z3, der=True)\n",
    "        delta_2 = np.dot(delta_3, w3) * ReLU_act(z2, der=True) # Second Layer Error\n",
    "        delta_1 = np.dot(delta_2, w2) * ReLU_act(z1, der=True)  # First Layer Error\n",
    "\n",
    "        # Gradient descent\n",
    "        wOut -= eta * np.outer(delta_Out, z3)  # Outer Layer\n",
    "        bOut -= eta * delta_Out\n",
    "        \n",
    "        w3 -= eta * np.outer(delta_3, z2)  # Hidden Layer 2\n",
    "        b3 -= eta * delta_3\n",
    "\n",
    "        w2 -= eta * np.outer(delta_2, z1)  # Hidden Layer 2\n",
    "        b2 -= eta * delta_2\n",
    "\n",
    "        w1 -= eta * np.outer(delta_1, x)  # Hidden Layer 1\n",
    "        b1 -= eta * delta_1\n",
    "\n",
    "        # Computação da função de perda (cross-entropy)\n",
    "        loss = -np.sum(y_true * np.log(y + 1e-9))\n",
    "        mu.append(loss)\n",
    "        vec_y.append(np.argmax(y))\n",
    "\n",
    "    # Previsão final em X_test\n",
    "    y_pred = np.array([np.argmax(softmax_act(np.dot(wOut, ReLU_act(np.dot(w2, ReLU_act(np.dot(w1, x) + b1)) + b2)) + bOut)) for x in X_test])\n",
    "\n",
    "    # Converte previsões para strings\n",
    "    y_pred_labels = [class_names[pred] for pred in y_pred]\n",
    "\n",
    "    # Cálculo da balanced accuracy\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred_labels)\n",
    "\n",
    "    # Matriz de confusão\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.2f}\")\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b119c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311bc329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e94bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af884f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a067dea2",
   "metadata": {},
   "source": [
    "Referências:  \n",
    "https://www.kaggle.com/code/androbomb/simple-nn-with-python-multi-layer-perceptron  \n",
    "https://medium.com/ensina-ai/rede-neural-perceptron-multicamadas-f9de8471f1a9  \n",
    "https://iaexpert.academy/2020/05/04/quantas-camadas-escondidas-e-quantos-neuronios-incluir-numa-rede-neural-artificial/\n",
    "https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c  \n",
    "https://github.com/enochkan/building-from-scratch/blob/main/optimizers/adam-optimizer-from-scratch.ipynb  \n",
    "https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cecba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a77f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
